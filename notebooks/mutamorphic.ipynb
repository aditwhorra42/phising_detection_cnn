{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.parse import urlparse, urlunparse\n",
    "from tensorflow.keras.models import load_model\n",
    "import numpy as np\n",
    "import re\n",
    "from ml_lib_remla.preprocessing import Preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_DATAPOINTS = 10\n",
    "DATASET_PATH = \"./../data/DL Dataset/test.txt\"\n",
    "MODEL_PATH = \"./../model/model.keras\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(data_path: str):\n",
    "    \"\"\"Loads the data split from the path. The path should be a .txt file that\n",
    "    has been created from the get_data step. his should be stored in the data folder.\n",
    "\n",
    "    Args:\n",
    "        data_path (str): The path to the split .txt file.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[List[str], List[str]]: Returns a tuple of raw_x and raw_y. raw_x is a\n",
    "        list of strings for all the sentences in the split and raw_y is their corresponding label.\n",
    "    \"\"\"\n",
    "    print(f\"Loading dataset: {data_path}\")\n",
    "\n",
    "    try:\n",
    "        with open(data_path, \"r\") as data_file:\n",
    "            loaded_data = [line.strip() for line in data_file.readlines()[1:]]\n",
    "    except FileNotFoundError as file_not_found_error:\n",
    "        raise FileNotFoundError(f\"Could not find file {data_path}.\") from file_not_found_error\n",
    "    except OSError as exception:\n",
    "        raise OSError(f\"An error occurred accessing file {data_path}: {exception}\") from exception\n",
    "\n",
    "    raw_x = [line.split(\"\\t\")[1] for line in loaded_data]\n",
    "    raw_y = [line.split(\"\\t\")[0] for line in loaded_data]\n",
    "    return raw_x, raw_y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset: ./../data/DL Dataset/test.txt\n"
     ]
    }
   ],
   "source": [
    "X_test, y_test = load_dataset(DATASET_PATH)\n",
    "X_test = X_test[:N_DATAPOINTS]\n",
    "y_test = y_test[:N_DATAPOINTS]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ParseResult(scheme='http', netloc='business.hsbc.com.camaract.mobi', path='/system_directory/isa/file.aspx', params='', query='session=61810342760774852870650739159893454615115912022877845677244', fragment=''), ParseResult(scheme='http', netloc='facebook.com-source-page.com', path='/help/contact/4359439512093023/', params='', query='', fragment=''), ParseResult(scheme='http', netloc='michaelnielsen.org', path='/blog/lecture-course-the-google-technology-stack/', params='', query='', fragment=''), ParseResult(scheme='http', netloc='messagerie-17fr.com', path='/fr/91da56ae94f5f4ff2b9dedcbcba90e2b/spg.php', params='', query='amp=&intid=8e63a4d8384a843ee3b10f5b5c48dfef&rnv=026', fragment=''), ParseResult(scheme='https', netloc='www.juventus.com', path='/wps/portal/en/news/diritti%20di%20opzione%2022giugno2011/!ut/p/b1/vzpfbtsgfmafzq_qca7ygc7txhgdgpwh7nw-idj1m-luys6qrfbtj2htpsykdtoniyehvspvowcghwkxajtsfxlia-ko25-779ux3fgwft6vo76hpjtvokgaollcqoo0szftskbo0j4faimmmarrciq0xgv1gfkgof2d_77nspdfi6c2fxrkj3wmburwpns9drptl2egmypbqubxzxcsemi403sxnhlljarlsqxjjr0vebxajrhc2wnvriwxzmdd-s2c0ztnv4krqvt9cfhkwiclbhfeicup4g1mp_5ip_1u9vo1un09qsnkcdgsbxww2sat0o22ixqwhknzlhobjyq-g43oiqemqqnwflt7o_mxia_-n5b-opdi9ggplmxruatmz-4d_gkoyvnkdk2x91kuyg6udcej1v4qvz8qahh1b42uh1hrsutftyaprtmr9vjygu3l0twvr4bcq7yp7bvwxqov2l8c3qoqpxx40xlmmk0zckbd7an33-zk6j4zfxvxqn87r4rihkdpvwdz-hsx/dl4/d5/l2dbisevz0fbis9nqseh/', params='', query='', fragment=''), ParseResult(scheme='http', netloc='www.asepib.ch', path='/MyUps/UPS.htm', params='', query='', fragment=''), ParseResult(scheme='http', netloc='www.paypal.com.x41bais2bzt8jz3.001ytg60u46w7x2tz1.com', path='/cgi-bin/webscr/', params='', query='login-dispatch&login_email=dresstokillbkk@yahoo.com&ref=pp&login-processing=ok', fragment=''), ParseResult(scheme='https', netloc='www.asthmaaustralia.org.au', path='/nt/research/participate-in-asthma-research', params='', query='', fragment=''), ParseResult(scheme='http', netloc='goopen.ps-host.tk', path='/', params='', query='', fragment=''), ParseResult(scheme='http', netloc='californiaimport.de', path='/administrator/cache/us.paypal.com/229b237701c47e8ee57168f098c2818c/', params='', query='', fragment='')]\n"
     ]
    }
   ],
   "source": [
    "parsed_urls = [urlparse(url) for url in X_test]\n",
    "print(parsed_urls[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_scheme(scheme):\n",
    "    \n",
    "    if scheme == 'http':\n",
    "        return 'https'\n",
    "    elif scheme == 'https':\n",
    "        return 'http'\n",
    "    else:\n",
    "        return scheme\n",
    "    \n",
    "def replace_tld(netloc):\n",
    "    \n",
    "    tld_list = ['.io', '.ai', '.dev']\n",
    "    tld_pattern = re.compile(r'\\.(com|org|de|net|uk|us|mobi|co\\.uk|gov|edu|io|ai|dev|biz|info|mil|int|arpa)\\b', re.IGNORECASE)\n",
    "\n",
    "    # Use regex to find and replace the TLD in the netloc\n",
    "    new_netloc = tld_pattern.sub(lambda match: '.' + np.random.choice(tld_list).lstrip('.'), netloc)\n",
    "\n",
    "    return new_netloc\n",
    "    \n",
    "\n",
    "parsed_urls_scheme = [url._replace(scheme=replace_scheme(url.scheme)) for url in parsed_urls]\n",
    "parsed_urls_scheme_tld = [url._replace(scheme=replace_tld(url.netloc)) for url in parsed_urls_scheme]\n",
    "mutated_urls = np.array([urlunparse(url) for url in parsed_urls_scheme_tld])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\tudelft\\remla24-team6\\phishing_detection_cnn\\.venv\\Lib\\site-packages\\sklearn\\base.py:376: InconsistentVersionWarning: Trying to unpickle estimator LabelEncoder from version 1.4.2 when using version 1.5.0. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "preprocessor = Preprocessing()\n",
    "\n",
    "X_original = preprocessor.tokenize_batch(X_test)\n",
    "y_original = preprocessor.encode_label_batch(y_test)\n",
    "\n",
    "X_mutator = preprocessor.tokenize_batch(mutated_urls)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = load_model(MODEL_PATH)\n",
    "original_predictions = model.predict(X_original)\n",
    "mutator_predictions = model.predict(X_mutator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "print(cosine_similarity(original_predictions, mutator_predictions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
