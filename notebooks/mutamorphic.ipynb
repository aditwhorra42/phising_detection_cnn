{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.parse import urlparse, urlunparse\n",
    "from tensorflow.keras.models import load_model\n",
    "import numpy as np\n",
    "\n",
    "import re\n",
    "from ml_lib_remla.preprocessing import Preprocessing\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_DATAPOINTS = 1000\n",
    "DATASET_PATH = \"./../data/DL Dataset/test.txt\"\n",
    "MODEL_PATH = \"./../model/model.keras\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(data_path: str):\n",
    "    \"\"\"Loads the data split from the path. The path should be a .txt file that\n",
    "    has been created from the get_data step. his should be stored in the data folder.\n",
    "\n",
    "    Args:\n",
    "        data_path (str): The path to the split .txt file.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[List[str], List[str]]: Returns a tuple of raw_x and raw_y. raw_x is a\n",
    "        list of strings for all the sentences in the split and raw_y is their corresponding label.\n",
    "    \"\"\"\n",
    "    print(f\"Loading dataset: {data_path}\")\n",
    "\n",
    "    try:\n",
    "        with open(data_path, \"r\") as data_file:\n",
    "            loaded_data = [line.strip() for line in data_file.readlines()[1:]]\n",
    "    except FileNotFoundError as file_not_found_error:\n",
    "        raise FileNotFoundError(f\"Could not find file {data_path}.\") from file_not_found_error\n",
    "    except OSError as exception:\n",
    "        raise OSError(f\"An error occurred accessing file {data_path}: {exception}\") from exception\n",
    "\n",
    "    raw_x = [line.split(\"\\t\")[1] for line in loaded_data]\n",
    "    raw_y = [line.split(\"\\t\")[0] for line in loaded_data]\n",
    "    return raw_x, raw_y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset: ./../data/DL Dataset/test.txt\n"
     ]
    }
   ],
   "source": [
    "X_test, y_test = load_dataset(DATASET_PATH)\n",
    "X_test = X_test[:N_DATAPOINTS]\n",
    "y_test = y_test[:N_DATAPOINTS]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 16ms/step\n",
      "[[  0   0]\n",
      " [  0  12]\n",
      " [  0  19]\n",
      " [  0  20]\n",
      " [  0  36]\n",
      " [  0  47]\n",
      " [  0  49]\n",
      " [  0  52]\n",
      " [  0  67]\n",
      " [  0  87]\n",
      " [  0 101]\n",
      " [  0 127]\n",
      " [  0 131]\n",
      " [  0 137]\n",
      " [  0 158]\n",
      " [  0 169]\n",
      " [  0 171]\n",
      " [  0 174]\n",
      " [  0 177]\n",
      " [  0 198]\n",
      " [  0 199]\n",
      " [  0 211]\n",
      " [  0 227]\n",
      " [  0 246]\n",
      " [  0 254]\n",
      " [  0 255]\n",
      " [  0 265]\n",
      " [  0 269]\n",
      " [  0 273]\n",
      " [  0 279]\n",
      " [  0 285]\n",
      " [  0 287]\n",
      " [  0 300]\n",
      " [  0 304]\n",
      " [  0 307]\n",
      " [  0 309]\n",
      " [  0 312]\n",
      " [  0 319]\n",
      " [  0 323]\n",
      " [  0 333]\n",
      " [  0 351]\n",
      " [  0 360]\n",
      " [  0 363]\n",
      " [  0 371]\n",
      " [  0 372]\n",
      " [  0 377]\n",
      " [  0 398]\n",
      " [  0 408]\n",
      " [  0 411]\n",
      " [  0 417]\n",
      " [  0 418]\n",
      " [  0 421]\n",
      " [  0 432]\n",
      " [  0 433]\n",
      " [  0 443]\n",
      " [  0 445]\n",
      " [  0 453]\n",
      " [  0 455]\n",
      " [  0 487]\n",
      " [  0 491]\n",
      " [  0 494]\n",
      " [  0 495]\n",
      " [  0 507]\n",
      " [  0 516]\n",
      " [  0 523]\n",
      " [  0 524]\n",
      " [  0 527]\n",
      " [  0 535]\n",
      " [  0 536]\n",
      " [  0 537]\n",
      " [  0 542]\n",
      " [  0 546]\n",
      " [  0 558]\n",
      " [  0 560]\n",
      " [  0 565]\n",
      " [  0 571]\n",
      " [  0 590]\n",
      " [  0 591]\n",
      " [  0 592]\n",
      " [  0 605]\n",
      " [  0 627]\n",
      " [  0 661]\n",
      " [  0 671]\n",
      " [  0 673]\n",
      " [  0 674]\n",
      " [  0 687]\n",
      " [  0 694]\n",
      " [  0 711]\n",
      " [  0 712]\n",
      " [  0 713]\n",
      " [  0 737]\n",
      " [  0 741]\n",
      " [  0 758]\n",
      " [  0 767]\n",
      " [  0 782]\n",
      " [  0 839]\n",
      " [  0 842]\n",
      " [  0 846]\n",
      " [  0 850]\n",
      " [  0 851]\n",
      " [  0 872]\n",
      " [  0 878]\n",
      " [  0 879]\n",
      " [  0 883]\n",
      " [  0 897]\n",
      " [  0 899]\n",
      " [  0 909]\n",
      " [  0 913]\n",
      " [  0 920]\n",
      " [  0 937]\n",
      " [  0 943]\n",
      " [  0 944]\n",
      " [  0 945]\n",
      " [  0 954]\n",
      " [  0 956]\n",
      " [  0 957]\n",
      " [  0 961]\n",
      " [  0 967]\n",
      " [  0 969]\n",
      " [  0 979]\n",
      " [  0 980]\n",
      " [  1   0]\n",
      " [  1  12]\n",
      " [  1  19]\n",
      " [  1  20]\n",
      " [  1  36]\n",
      " [  1  47]\n",
      " [  1  49]\n",
      " [  1  52]\n",
      " [  1  55]\n",
      " [  1  67]\n",
      " [  1  87]\n",
      " [  1 101]\n",
      " [  1 127]\n",
      " [  1 137]\n",
      " [  1 158]\n",
      " [  1 169]\n",
      " [  1 171]\n",
      " [  1 174]\n",
      " [  1 177]\n",
      " [  1 180]\n",
      " [  1 198]\n",
      " [  1 199]\n",
      " [  1 211]\n",
      " [  1 227]\n",
      " [  1 246]\n",
      " [  1 254]\n",
      " [  1 255]\n",
      " [  1 265]\n",
      " [  1 269]\n",
      " [  1 273]\n",
      " [  1 287]\n",
      " [  1 300]\n",
      " [  1 304]\n",
      " [  1 307]\n",
      " [  1 309]\n",
      " [  1 312]\n",
      " [  1 319]\n",
      " [  1 323]\n",
      " [  1 333]\n",
      " [  1 351]\n",
      " [  1 360]\n",
      " [  1 371]\n",
      " [  1 377]\n",
      " [  1 398]\n",
      " [  1 408]\n",
      " [  1 411]\n",
      " [  1 417]\n",
      " [  1 418]\n",
      " [  1 421]\n",
      " [  1 432]\n",
      " [  1 433]\n",
      " [  1 443]\n",
      " [  1 445]\n",
      " [  1 453]\n",
      " [  1 455]\n",
      " [  1 487]\n",
      " [  1 491]\n",
      " [  1 494]\n",
      " [  1 495]\n",
      " [  1 507]\n",
      " [  1 516]\n",
      " [  1 523]\n",
      " [  1 524]\n",
      " [  1 527]\n",
      " [  1 535]\n",
      " [  1 536]\n",
      " [  1 537]\n",
      " [  1 542]\n",
      " [  1 546]\n",
      " [  1 558]\n",
      " [  1 560]\n",
      " [  1 571]\n",
      " [  1 590]\n",
      " [  1 591]\n",
      " [  1 592]\n",
      " [  1 605]\n",
      " [  1 661]\n",
      " [  1 671]\n",
      " [  1 673]\n",
      " [  1 674]\n",
      " [  1 687]\n",
      " [  1 694]\n",
      " [  1 709]\n",
      " [  1 711]\n",
      " [  1 712]\n",
      " [  1 713]\n",
      " [  1 741]\n",
      " [  1 758]\n",
      " [  1 767]\n",
      " [  1 782]\n",
      " [  1 799]\n",
      " [  1 801]\n",
      " [  1 839]\n",
      " [  1 846]\n",
      " [  1 850]\n",
      " [  1 851]\n",
      " [  1 872]\n",
      " [  1 878]\n",
      " [  1 879]\n",
      " [  1 883]\n",
      " [  1 897]\n",
      " [  1 899]\n",
      " [  1 909]\n",
      " [  1 913]\n",
      " [  1 920]\n",
      " [  1 929]\n",
      " [  1 937]\n",
      " [  1 943]\n",
      " [  1 944]\n",
      " [  1 945]\n",
      " [  1 954]\n",
      " [  1 956]\n",
      " [  1 957]\n",
      " [  1 961]\n",
      " [  1 967]\n",
      " [  1 969]\n",
      " [  1 979]\n",
      " [  1 980]\n",
      " [  2   0]\n",
      " [  2  12]\n",
      " [  2  19]\n",
      " [  2  20]\n",
      " [  2  36]\n",
      " [  2  49]\n",
      " [  2  52]\n",
      " [  2  55]\n",
      " [  2  67]\n",
      " [  2  87]\n",
      " [  2  91]\n",
      " [  2 101]\n",
      " [  2 127]\n",
      " [  2 131]\n",
      " [  2 137]\n",
      " [  2 158]\n",
      " [  2 169]\n",
      " [  2 171]\n",
      " [  2 174]\n",
      " [  2 177]\n",
      " [  2 180]\n",
      " [  2 199]\n",
      " [  2 211]\n",
      " [  2 227]\n",
      " [  2 246]\n",
      " [  2 253]\n",
      " [  2 254]\n",
      " [  2 255]\n",
      " [  2 261]\n",
      " [  2 265]\n",
      " [  2 269]\n",
      " [  2 273]\n",
      " [  2 279]\n",
      " [  2 287]\n",
      " [  2 300]\n",
      " [  2 304]\n",
      " [  2 307]\n",
      " [  2 309]\n",
      " [  2 312]\n",
      " [  2 323]\n",
      " [  2 333]\n",
      " [  2 351]\n",
      " [  2 360]\n",
      " [  2 363]\n",
      " [  2 364]\n",
      " [  2 371]\n",
      " [  2 375]\n",
      " [  2 377]\n",
      " [  2 398]\n",
      " [  2 408]\n",
      " [  2 411]\n",
      " [  2 418]\n",
      " [  2 421]\n",
      " [  2 426]\n",
      " [  2 432]\n",
      " [  2 433]\n",
      " [  2 443]\n",
      " [  2 445]\n",
      " [  2 447]\n",
      " [  2 450]\n",
      " [  2 453]\n",
      " [  2 455]\n",
      " [  2 463]\n",
      " [  2 487]\n",
      " [  2 491]\n",
      " [  2 494]\n",
      " [  2 495]\n",
      " [  2 503]\n",
      " [  2 507]\n",
      " [  2 516]\n",
      " [  2 523]\n",
      " [  2 524]\n",
      " [  2 527]\n",
      " [  2 529]\n",
      " [  2 535]\n",
      " [  2 536]\n",
      " [  2 537]\n",
      " [  2 542]\n",
      " [  2 546]\n",
      " [  2 558]\n",
      " [  2 560]\n",
      " [  2 565]\n",
      " [  2 571]\n",
      " [  2 587]\n",
      " [  2 590]\n",
      " [  2 591]\n",
      " [  2 592]\n",
      " [  2 600]\n",
      " [  2 605]\n",
      " [  2 627]\n",
      " [  2 628]\n",
      " [  2 661]\n",
      " [  2 671]\n",
      " [  2 672]\n",
      " [  2 674]\n",
      " [  2 689]\n",
      " [  2 694]\n",
      " [  2 705]\n",
      " [  2 709]\n",
      " [  2 711]\n",
      " [  2 712]\n",
      " [  2 713]\n",
      " [  2 741]\n",
      " [  2 758]\n",
      " [  2 782]\n",
      " [  2 788]\n",
      " [  2 799]\n",
      " [  2 801]\n",
      " [  2 816]\n",
      " [  2 831]\n",
      " [  2 839]\n",
      " [  2 846]\n",
      " [  2 850]\n",
      " [  2 851]\n",
      " [  2 866]\n",
      " [  2 872]\n",
      " [  2 878]\n",
      " [  2 879]\n",
      " [  2 883]\n",
      " [  2 897]\n",
      " [  2 899]\n",
      " [  2 909]\n",
      " [  2 913]\n",
      " [  2 920]\n",
      " [  2 929]\n",
      " [  2 937]\n",
      " [  2 943]\n",
      " [  2 944]\n",
      " [  2 945]\n",
      " [  2 953]\n",
      " [  2 954]\n",
      " [  2 956]\n",
      " [  2 957]\n",
      " [  2 961]\n",
      " [  2 967]\n",
      " [  2 969]\n",
      " [  2 979]\n",
      " [  2 980]]\n"
     ]
    }
   ],
   "source": [
    "def test_input_generation(urls):\n",
    "    # Parse the URLs from the input list\n",
    "    parsed_urls = [urlparse(url) for url in X_test]\n",
    "\n",
    "    def replace_scheme(scheme):\n",
    "        # Replace the scheme of the URL\n",
    "        if scheme == 'http':\n",
    "            return 'https'\n",
    "        elif scheme == 'https':\n",
    "            return 'http'\n",
    "        else:\n",
    "            return scheme\n",
    "\n",
    "    def replace_tld(netloc, tld):\n",
    "        # Replace the top-level domain (TLD) in the netloc\n",
    "        tld_pattern = re.compile(r'\\.(com|org|de|net|uk|us|mobi|co\\.uk|gov|edu|io|ai|dev|biz|info|mil|int|arpa)\\b', re.IGNORECASE)\n",
    "        new_netloc = tld_pattern.sub(lambda match: '.' + tld.lstrip('.'), netloc)\n",
    "        return new_netloc\n",
    "\n",
    "    tld_list = ['.io', '.ai', '.dev']\n",
    "\n",
    "    # Generate mutant candidates by replacing scheme and TLD\n",
    "    parsed_urls_scheme = [url._replace(scheme=replace_scheme(url.scheme)) for url in parsed_urls]\n",
    "    parsed_urls_scheme_tld = [url._replace(scheme=replace_tld(url.netloc, tld=tld)) for url in parsed_urls_scheme for tld in tld_list]\n",
    "\n",
    "    # Filter out empty mutants\n",
    "    mutated_urls = np.array([urlunparse(url) for url in parsed_urls_scheme_tld if urlunparse(url) != \"\"])\n",
    "\n",
    "    return mutated_urls\n",
    "\n",
    "def test_oracle_generation(y_pred_original, y_pred_mutant, threshold=0.5):\n",
    "    n_mutants = len(y_pred_mutant) // len(y_pred_original)\n",
    "    y_pred_original = y_pred_original.flatten()\n",
    "    y_pred_mutant = y_pred_mutant.reshape(len(y_pred_original), n_mutants).T\n",
    "\n",
    "    labels_original = (np.array(y_pred_original) > threshold).astype(int)\n",
    "    labels_mutant = (np.array(y_pred_mutant) > threshold).astype(int)\n",
    "    \n",
    "    failing_tests = np.argwhere(labels_original != labels_mutant)\n",
    "    \n",
    "    return failing_tests \n",
    "\n",
    "def test_mutamorphic(X_orig):\n",
    "    # Generate mutant candidates\n",
    "    mutant_candidates = test_input_generation(X_orig)\n",
    "\n",
    "    preprocessor = Preprocessing()\n",
    "    X_orig = preprocessor.tokenize_batch(X_orig)\n",
    "    X_mutator = preprocessor.tokenize_batch(mutant_candidates)\n",
    "\n",
    "    model = load_model(MODEL_PATH)\n",
    "    y_pred_original = model.predict(X_orig)\n",
    "    y_pred_mutant = model.predict(X_mutator)\n",
    "\n",
    "    failing_tests = test_oracle_generation(y_pred_original, y_pred_mutant)\n",
    "    \n",
    "    return failing_tests\n",
    "\n",
    "# Perform mutamorphic testing and assert the number of failing tests\n",
    "failing_tests = test_mutamorphic(X_orig=X_test)\n",
    "print(failing_tests)\n",
    "assert len(failing_tests) < len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
