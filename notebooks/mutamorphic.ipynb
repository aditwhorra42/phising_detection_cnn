{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.parse import urlparse, urlunparse\n",
    "from tensorflow.keras.models import load_model\n",
    "import numpy as np\n",
    "\n",
    "import re\n",
    "from ml_lib_remla.preprocessing import Preprocessing\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_DATAPOINTS = 5000\n",
    "DATASET_PATH = \"./../data/DL Dataset/test.txt\"\n",
    "MODEL_PATH = \"./../model/model.keras\"\n",
    "\n",
    "\n",
    "N_SAMPLES = 100\n",
    "TLD_LIST = ['.io', '.ai', '.dev']\n",
    "TLD_REPAIR_LIST = ['.uk', '.org', '.de']\n",
    "\n",
    "THRESHOLD = 0.5\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(data_path: str):\n",
    "    \"\"\"Loads the data split from the path. The path should be a .txt file that\n",
    "    has been created from the get_data step. his should be stored in the data folder.\n",
    "\n",
    "    Args:\n",
    "        data_path (str): The path to the split .txt file.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[List[str], List[str]]: Returns a tuple of raw_x and raw_y. raw_x is a\n",
    "        list of strings for all the sentences in the split and raw_y is their corresponding label.\n",
    "    \"\"\"\n",
    "    print(f\"Loading dataset: {data_path}\")\n",
    "\n",
    "    try:\n",
    "        with open(data_path, \"r\") as data_file:\n",
    "            loaded_data = [line.strip() for line in data_file.readlines()[1:]]\n",
    "    except FileNotFoundError as file_not_found_error:\n",
    "        raise FileNotFoundError(f\"Could not find file {data_path}.\") from file_not_found_error\n",
    "    except OSError as exception:\n",
    "        raise OSError(f\"An error occurred accessing file {data_path}: {exception}\") from exception\n",
    "\n",
    "    raw_x = [line.split(\"\\t\")[1] for line in loaded_data]\n",
    "    raw_y = [line.split(\"\\t\")[0] for line in loaded_data]\n",
    "    return raw_x, raw_y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset: ./../data/DL Dataset/test.txt\n"
     ]
    }
   ],
   "source": [
    "X_test, y_test = load_dataset(DATASET_PATH)\n",
    "X_test = np.array(X_test[3500:N_DATAPOINTS])\n",
    "y_test = np.array(y_test[3500:N_DATAPOINTS])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['http://jxnblk.com/stepkit/'\n",
      " 'http://businessbanking.53.com.session2120332.versuse.cn/clientbase/form.asp'\n",
      " 'http://www.abogadosbyg.cl/Accountpaypalaccsecuresummarytoken115323redirectupdateinfo/?cmd=_home&dispatch=5885d80a13c0db1f8e&ee=da61bdbd5191abca069324270ceaed1a'\n",
      " ... 'https://www.justeasy.cn/3d/id-341682.html'\n",
      " 'http://www.usaa.com.kjlkjqw.org.uk/inet/ent_formversionnew/do_action?id=62147325999120009511348897229802282465531106735919176484110704'\n",
      " 'http://hapoalim.co.il.vvc1.in/webscr.php']\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 28ms/step\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 23ms/step\n",
      "['signin.ebay.io.513422301.513422301.513422301.513422301.513422.7fdeerewreg4rkjw4eergererwx.io://signin.ebay.com.513422301.513422301.513422301.513422301.513422.7fdeerewreg4rkjw4eergererwx.com/sc/saw-cgi/eBayISAPI.dll/'\n",
      " 'signin.ebay.ai.513422301.513422301.513422301.513422301.513422.7fdeerewreg4rkjw4eergererwx.ai://signin.ebay.com.513422301.513422301.513422301.513422301.513422.7fdeerewreg4rkjw4eergererwx.com/sc/saw-cgi/eBayISAPI.dll/'\n",
      " 'signin.ebay.dev.513422301.513422301.513422301.513422301.513422.7fdeerewreg4rkjw4eergererwx.dev://signin.ebay.com.513422301.513422301.513422301.513422301.513422.7fdeerewreg4rkjw4eergererwx.com/sc/saw-cgi/eBayISAPI.dll/']\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n"
     ]
    }
   ],
   "source": [
    "def input_generation(urls, tld_list = TLD_LIST):\n",
    "    # Parse the URLs from the input list\n",
    "    print(urls)\n",
    "    parsed_urls = [urlparse(url) for url in urls]\n",
    "\n",
    "    def replace_scheme(scheme):\n",
    "        # Replace the scheme of the URL\n",
    "        if scheme == 'http':\n",
    "            return 'https'\n",
    "        elif scheme == 'https':\n",
    "            return 'http'\n",
    "        else:\n",
    "            return scheme\n",
    "\n",
    "    def replace_tld(netloc, tld):\n",
    "        # Replace the top-level domain (TLD) in the netloc\n",
    "        tld_pattern = re.compile(r'\\.(com|org|de|net|uk|us|mobi|co\\.uk|gov|edu|io|ai|dev|biz|info|mil|int|arpa)\\b', re.IGNORECASE)\n",
    "        new_netloc = tld_pattern.sub(lambda match: '.' + tld.lstrip('.'), netloc)\n",
    "        return new_netloc\n",
    "\n",
    "    # Generate mutant candidates by replacing scheme and TLD\n",
    "    parsed_urls_scheme = [url._replace(scheme=replace_scheme(url.scheme)) for url in parsed_urls]\n",
    "    parsed_urls_scheme_tld = [url._replace(scheme=replace_tld(url.netloc, tld=tld)) for url in parsed_urls_scheme for tld in tld_list]\n",
    "\n",
    "    # Filter out empty mutants\n",
    "    mutated_urls = np.array([urlunparse(url) for url in parsed_urls_scheme_tld if urlunparse(url) != \"\"])\n",
    "\n",
    "    return mutated_urls\n",
    "\n",
    "def get_labels(y_pred_original, y_pred_mutant, threshold=0.5):\n",
    "    labels_original = (np.array(y_pred_original) > threshold).astype(int)\n",
    "    labels_mutants = (np.array(y_pred_mutant) > threshold).astype(int)\n",
    "    \n",
    "    labels_mutant = np.max(labels_mutants, axis=0)\n",
    "    \n",
    "    return labels_original, labels_mutant\n",
    "\n",
    "def oracle_generation(y_pred_original, y_pred_mutant):\n",
    "    n_mutants = len(y_pred_mutant) // len(y_pred_original)\n",
    "    y_pred_original = y_pred_original.flatten()\n",
    "    y_pred_mutant = y_pred_mutant.reshape(len(y_pred_original), n_mutants).T\n",
    "\n",
    "    labels_original, labels_mutant = get_labels(y_pred_original, y_pred_mutant)\n",
    "    failing_tests = np.argwhere(labels_original != labels_mutant)\n",
    "    \n",
    "    return failing_tests \n",
    "\n",
    "def automatic_repair(model,preprocessor, X_failing_mutants, y_prob_original):\n",
    "    \n",
    "    mutant_candidates = input_generation(X_failing_mutants, tld_list=TLD_REPAIR_LIST)\n",
    "    \n",
    "    X_mutants = preprocessor.tokenize_batch(mutant_candidates)\n",
    "    y_prob_mutants = model.predict(X_mutants)\n",
    "    y_prob_mutants.reshape(len(X_failing_mutants), len(TLD_REPAIR_LIST))\n",
    "    \n",
    "    labels_original, labels_repaired = get_labels(y_pred_original=y_prob_original, y_pred_mutant=y_prob_mutants)\n",
    "    labels_final = np.equal(labels_original,labels_repaired)\n",
    "    \n",
    "    \n",
    "    return labels_final\n",
    "\n",
    "def test_mutamorphic(X_orig):\n",
    "    # Generate mutant candidates\n",
    "    mutant_candidates = input_generation(X_orig)\n",
    "\n",
    "    preprocessor = Preprocessing()\n",
    "    X_orig_processed = preprocessor.tokenize_batch(X_orig)\n",
    "    X_mutants = preprocessor.tokenize_batch(mutant_candidates)\n",
    "    model = load_model(MODEL_PATH)\n",
    "    y_prob_original = model.predict(X_orig_processed)\n",
    "    y_pred_mutants = model.predict(X_mutants)\n",
    "    \n",
    "    failing_tests = oracle_generation(y_prob_original, y_pred_mutants)\n",
    "\n",
    "    X_failing_mutants = mutant_candidates.reshape(len(X_orig), len(TLD_LIST))\n",
    "    y_prob_mutants = y_pred_mutants.reshape(len(X_orig), len(TLD_LIST))\n",
    "    X_failing_mutants = X_failing_mutants[failing_tests].flatten()\n",
    "    y_prob_mutants = y_prob_mutants[failing_tests]\n",
    "    \n",
    "    labels_final = automatic_repair(model, preprocessor=preprocessor, X_failing_mutants=X_failing_mutants, y_prob_original=y_prob_original)\n",
    "    \n",
    "    return labels_final\n",
    "\n",
    "# Perform mutamorphic testing and assert the number of failing tests.\n",
    "failing_tests = test_mutamorphic(X_orig=X_test)\n",
    "# apply heuristic to get less than 10% wrong labels.\n",
    "assert np.sum(failing_tests) > len(y_test) // 1.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
